{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"X does not have valid feature names, but [a-zA-Z]+ was fitted with feature names\",\n",
        "    category=UserWarning,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings = lambda *a, **kw: None\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "HTML(open(\"custom.html\", \"r\").read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 5: Preprocessing, pipelines and hyperparameters optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Types of data transformations / preprocessing\n",
        "\n",
        "Note: we use terms data *transformation* and *preprocessing* interchangeably.\n",
        "\n",
        "We've seen before that adding polynomial features to the 2D `xor` and `circle` problem made both tasks treatable by a simple linear classifier.\n",
        "\n",
        "Beyond adding polynomial features, there are other important preprocessors / transformers to know of."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaler\n",
        "\n",
        "A scaler applies a linear transformation on every feature. Those transformations are individual per column.\n",
        "\n",
        "The two most important scalers in `sklearn.preprocessing` module are:\n",
        "\n",
        "- `MinMaxScaler`: after applying this scaler, the minumum in every column is 0, the maximum is 1.\n",
        "\n",
        "- `StandardScaler`: scales columns to mean value 0 and standard deviation 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why scaling?\n",
        "\n",
        "Let us assume we have two features `x` and `y` with different ranges: `x` is in the range `0` to `3` and `y` in `0` to `1`: We have sampled both variables with the same resolution:\n",
        "\n",
        "<img src=\"images/different_scales.png\">\n",
        "\n",
        "Some classifiers like `SVC` and `KNeighborsClassifier` use Euclidian distances between features internally. They assume that features which are close in distance also have similar classes or target values.\n",
        "\n",
        "So let us check what features have an Euclidian distance to `(1.5, 0.5)` below `0.2`:\n",
        "\n",
        "<img src=\"images/before_rescaling.png\"/>\n",
        "\n",
        "As you can see the points in the circle cover a big range of `y` values, but only a few points from `x`. You can assume that the feature `y` has a strong influence on how a classifier works, and `x` has less influence.\n",
        "\n",
        "You can also see that on absolute scales the circle is not a circle:\n",
        "\n",
        "<img src=\"images/before_rescaling_skewed_circle.png\"/>\n",
        "\n",
        "\n",
        "When we scale `x` to the range `0` to `1` the situation is as follows:\n",
        "\n",
        "<img src=\"images/after_rescaling.png\" >\n",
        "\n",
        "\n",
        "The reason to use a scaler is to compensate\n",
        "\n",
        "- for different orders of magnitudes\n",
        "- and physical units\n",
        "\n",
        "of the features.\n",
        "\n",
        "We say that a machine learning methods or transformations is **scale invariant** when rescaling (i.e. multiplying) any of the features (columns) by a nonzero number does not change predictions or transformation results, but may change the method or transformation itself.\n",
        "\n",
        "For instance, a simple linear classifier `weight_x * x + weight_y * y = threshold` is scale invariant because when changing unit a feature e.g. from centimeters to meters you divide feature values by `100`, but multiplying a feature weight by `100` gives the exact same model: `(weight_x * 100) * (x / 100) + weight_y * y = threshold`.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<p><i class=\"fa fa-warning\"></i>\n",
        "    <strong>Always (re)scale features</strong> before using non-scale invariant:\n",
        "</p>\n",
        "<ul>\n",
        "    <li>distance-based methods, like <code>SVC</code> or <code>KNeighborsClassifier</code>,</li>\n",
        "    <li>non-linear transformations or methods using such transformations, like <code>PolynomialFeatures</code> or <code>LogisticRegression</code> classifier,</li>\n",
        "    <li>otherwise, non-linearly scale-dependent transformations or methods, like <code>PCA</code>, or <code>l1</code> and <code>l2</code> regularization penalty-based methods, like <code>Ridge</code> or <code>Lasso</code> regression.</li>\n",
        "</ul>\n",
        "<p><i class=\"fa fa-warning\"></i>\n",
        "    <strong>Never (re)scale</strong> your features if they are already in the same scale (e.g. pixel values in an image), as any almost-constant unimportant features may gain importance.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimensionality reduction (PCA)\n",
        "\n",
        "Reducing the dimensionality of a multi variate data set removes redundancies in it, such as highly correlated columns. We've discussed before that reducing redundancy and noise can help to avoid overfitting.\n",
        "\n",
        "One of the most effective techniques for dimensionality reduction is a Principal Component Analysis (PCA). Its biggest downside is that the resulting few new features (principal components) cannot be directly interpreted in terms of original features.\n",
        "\n",
        "<table>\n",
        "    <tr><td><img src=\"images/pca-3d_to_2d.jpg\" width=\"725px\"/></td></tr>\n",
        "    <tr><td><center><sub>Source: <a href=\"https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e\">https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e</a></sub></center></td></tr>\n",
        "</table>\n",
        "\n",
        "The `sklearn.decomposition` module contains the standard `PCA` utility, as well as many of its variants and other dimensionality reduction techniques, and some more general features matrix decomposition techniques.\n",
        "\n",
        "**Don't forget to scale features before using PCA.**\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<p><i class=\"fa fa-info-circle\"></i>\n",
        "Dimensional reduction using PCA consists of finding the features that maximize the variance. If one feature varies more than the others only because of their respective scales, PCA would determine that such feature dominates the direction of the principal components. Thus, in order to work correctly PCA requires features to be in the same scale, i.e. with comparable (unit) variances. To that end, one typically uses the <code>StandardScaler</code> transformation. Alternatively, use <code>RobustScaler</code> when dealing with outliers or <code>MinMaxScaler</code> when it is required to capture a small variance in features. More information and examples can be found here: <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#effect-of-rescaling-on-a-pca-dimensional-reduction\">https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#effect-of-rescaling-on-a-pca-dimensional-reduction</a>\n",
        "<p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function transformers\n",
        "\n",
        "It can help to apply functions like `log` or `exp` or `1/x` to features to improve classification performance.\n",
        "\n",
        "Lets assume you want to forecast the outcome of car crash experiments and one variable is the time $t$ needed for the distance $l$ from start to crash. Transforming this to the actual speed $\\frac{l}{t}$ could be a more informative feature then $t$.\n",
        "\n",
        "Use a `FunctionTransformer` utility from `sklearn.preprocessing` to define and apply a function transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing values imputers\n",
        "\n",
        "Sometimes data contain missing values. Data imputation is a strategy to fill up missing values. Standard in statistics Missing (Completely/Not) At Random (MAR; MCAR; MNAR) approaches are not well-suited for machine learning tasks. Instead, in `sklearn.impute` module you will find:\n",
        "\n",
        "* `SimpleImputer`: columnwise mean/median/most frequent value approach that works great with good classifier and a lot of non-missing data, otherwise use\n",
        "* (semi-supervised) machine learning imputers:\n",
        "    * `KNNImputer`: mean value from k-Nearest Neighbors (closest samples by non-missing feature values); note: do scale features before using it,\n",
        "    * `IterativeImputer`: regresses each feature with missing values on other features, in an iterated round-robin fashion over each feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A scaling example\n",
        "\n",
        "As an example we demonstrante how a scaler can be implemented. Our scaling strategy will scale given values to the range 0 to 1.\n",
        "\n",
        "First we create a random data matrix and compute columnwise min and max values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# for reproducible numbers:\n",
        "np.random.seed(42)\n",
        "\n",
        "values = np.random.random((5,)) * 20 - 10\n",
        "\n",
        "min_value = values.min()\n",
        "max_value = values.max()\n",
        "\n",
        "print(\"values:\", values)\n",
        "print()\n",
        "print(\"min value:\", min_value)\n",
        "print(\"max value:\", max_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The strategy for scaling is as follows: Our values $v$ are in the range $v_{min}$ to $v_{max}$:\n",
        "\n",
        "$$\n",
        "v_{min} \\le  v  \\le v_{max}\n",
        "$$\n",
        "\n",
        "\n",
        "Then subtracting $v_{min}$ results in \n",
        "\n",
        "$$\n",
        "0 \\le  v - v_{min} \\le v_{max} - v_{min}\n",
        "$$\n",
        "  \n",
        "Finally dividing by the right hand side delivers the property we are looking for:\n",
        "\n",
        "$$\n",
        "0 \\le \\frac{v - v_{min}}{v_{max} - v_{min}} \\le 1\n",
        "$$\n",
        "\n",
        "\n",
        "In Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "scaled_values = (values - min_value) / (max_value - min_value)\n",
        "\n",
        "print(\"scaled values:\", scaled_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that all values are now scaled as intended.\n",
        "\n",
        "To apply the same strategy column per column to a feature matrix, `scikit-learn` offers a `MinMaxScaler`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "features = np.random.random((5, 3)) * 20 - 10\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1. learning -> determine columnwise min/max values\n",
        "scaler = MinMaxScaler().fit(features)\n",
        "\n",
        "# 2. transformation -> apply linear transformation based on min/max values:\n",
        "print(scaler.transform(features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 1+2. shorter\n",
        "print(scaler.fit_transform(features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to correctly transform data?\n",
        "\n",
        "We can divide preprocessing into two classes:\n",
        "\n",
        "1. Preprocessing which depends on the **full feature matrix** (including column-wise preprocessing). E.g.\n",
        "\n",
        "   - Dimensionality reduction (PCA)\n",
        "   - Column-wise functional transforms or scaling\n",
        "   - Imputation of missing values\n",
        "\n",
        "2. Preprocessing which can be applied **row per row individually**. E.g.\n",
        "\n",
        "   - Adding polynomial features\n",
        "   - Row-wise functional transforms or scaling (e.g. when a row represents an image and we want to compensate for different illumination)\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <p style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Data preprocessing commandments<p>\n",
        "    <ol>\n",
        "        <li>When we include preprocessing in a classification approach, <strong>we must apply exactly the same preprocessing on new incoming data</strong>!</li>\n",
        "        <li>This implies that <strong>for preprocessors which depend on the full data set we must never preprocess data before cross-validation</strong>!</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "Running preprocessors on the full dataset before cross-validation lets information of \"unseen\" test data sneak into the classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./images/2xi5wt.jpg\" width=50%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 150%;\"> This is how we must proceed</p>\n",
        "\n",
        "In case for the `MinMaxScaler` transformer:\n",
        "\n",
        "1. Determine column-wise minimum und maximum values of the training features.\n",
        "2. Use these min/max values to scale training data.\n",
        "3. Learn classifier `C` on the scaled training data.\n",
        "\n",
        "\n",
        "4. Use values from 1. to scale evaluation data (thus, we might create values outside `0..1`).\n",
        "5. Apply classifier `C` to the scaled evaluation data.\n",
        "6. Assess `C` performance.\n",
        "\n",
        "In general:\n",
        "\n",
        "1. Learn transformer `P` on the training data.\n",
        "2. Apply `P` to the training data.\n",
        "3. Learn classifier `C` on the transformed training data.\n",
        "\n",
        "\n",
        "4. Apply `P` from 1. to the evaluation data.\n",
        "5. Apply classifier `C` to the transformed evaluation data.\n",
        "6. Assess `C` performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "\n",
        "## Take-home message: Preprocessing    \n",
        "\n",
        "| Transformation | When is it applied? |\n",
        "| ------ | ------ |\n",
        "| Scaling | - bring features to same value range <br> - especially important when using models which use Euclidean distance between features | \n",
        "| PCA | - dimensionality reduction of multi-dimensional dataset  |\n",
        "| Function transformer | - transform present feature into more informative feature  |\n",
        "| Missing values imputers | - when single datapoints are missing |\n",
        "\n",
        "- **Dependency of preprocessing methods on the dataset**: Preprocessing methods either depend on the full feature matrix or only on single rows\n",
        "- **Order of data transformations**: Always split your data into train and evaluation set and then apply preprocessing and model training.\n",
        "\n",
        "    \n",
        "<p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The scikit-learn API (quick recap)\n",
        "\n",
        "We've seen before that we can swap `scikit-learn` classifiers easily without changing much code. \n",
        "\n",
        "This is possible, because all classifiers have methods `.fit` and `.predict` which also have the same function signature (this means number and meaning of arguments is always the same for every implementation of `.fit` respectively `.predict`.)\n",
        "\n",
        "This consistent design within `scikit-learn` also applies to data transformers, which all have methods`.fit`, `.transform` and `.fit_transform`.\n",
        "\n",
        "This consistent API allows setting up easily processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipelines\n",
        "\n",
        "A so called **classification pipeline consists of 0 or more data transformers plus a final classifier**.\n",
        "\n",
        "Let us start with the following pipeline:\n",
        "\n",
        "1. Apply scaling to mean 0 and std deviation 1\n",
        "2. Use PCA to reduce data to 3 dimensions\n",
        "3. Train `SVC` classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "p = make_pipeline(StandardScaler(), PCA(3), SVC())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<p><i class=\"fa fa-info-circle\"></i>\n",
        "A pipeline \"behaves\" like a single classifier - it implements <code>.fit()</code> and <code>.predict()</code> methods.</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"p.fit    \", p.fit is not None)\n",
        "print(\"p.predict\", p.predict is not None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because of this we can also use cross-validation in the same way as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "beer_data = pd.read_csv(\"data/beers.csv\")\n",
        "\n",
        "features = beer_data.iloc[:, :-1]\n",
        "labels = beer_data.iloc[:, -1]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "print(cross_val_score(p, features, labels, scoring=\"accuracy\", cv=5).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "<i class=\"fa fa-info-circle\"></i>&nbsp;One benefit of using a pipeline is that you will not mistakenly scale the full data set first, instead we follow the strategy we've described above automatically.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-10T17:50:34.626895Z",
          "iopub.status.busy": "2021-02-10T17:50:34.626575Z",
          "iopub.status.idle": "2021-02-10T17:50:34.640455Z",
          "shell.execute_reply": "2021-02-10T17:50:34.637491Z",
          "shell.execute_reply.started": "2021-02-10T17:50:34.626860Z"
        }
      },
      "source": [
        "Bonus: you can easily visualize a more complex pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "\n",
        "set_config(display=\"diagram\")\n",
        "p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to setup a good pipeline?\n",
        "\n",
        "Regrettably there is no recipe how to setup a good performing classification pipeline except reasonable preprocessing, especially feature engineering. After that it is up to experimentation and the advice on how to choose classifiers we gave in the last script.\n",
        "\n",
        "Let us try out different pipeplines and evaluate them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "for p in [\n",
        "    make_pipeline(KNeighborsClassifier()),\n",
        "    make_pipeline(MinMaxScaler(), KNeighborsClassifier()),\n",
        "    make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
        "    make_pipeline(SVC()),\n",
        "    make_pipeline(StandardScaler(), SVC()),\n",
        "    make_pipeline(LogisticRegression()),\n",
        "    make_pipeline(StandardScaler(), LogisticRegression()),\n",
        "    make_pipeline(StandardScaler(), PCA(2), LogisticRegression()),\n",
        "    make_pipeline(StandardScaler(), PCA(3), LogisticRegression()),\n",
        "    make_pipeline(PolynomialFeatures(2), SVC()),\n",
        "    make_pipeline(StandardScaler(), PolynomialFeatures(2), SVC()),\n",
        "    make_pipeline(StandardScaler(), PolynomialFeatures(3), SVC()),\n",
        "    # tech: max_iter to prevent a convergence warning\n",
        "    make_pipeline(PolynomialFeatures(2), LogisticRegression(max_iter=10000)),\n",
        "    make_pipeline(StandardScaler(), PolynomialFeatures(2), LogisticRegression()),\n",
        "]:\n",
        "\n",
        "    print(\n",
        "        \"{:.3f}\".format(\n",
        "            cross_val_score(p, features, labels, scoring=\"accuracy\", cv=5).mean()\n",
        "        ),\n",
        "        end=\" \",\n",
        "    )\n",
        "    print([pi[1] for pi in p.steps])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise session\n",
        "\n",
        "1. Can you come up with a better performing classification pipeline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "\n",
        "for p in [\n",
        "    # previouly best pipelines\n",
        "    make_pipeline(StandardScaler(), PolynomialFeatures(), LogisticRegression()),\n",
        "    make_pipeline(StandardScaler(), SVC()),\n",
        "    # bit better\n",
        "    make_pipeline(\n",
        "        StandardScaler(), PolynomialFeatures(), PCA(10), LogisticRegression()\n",
        "    ),\n",
        "    make_pipeline(StandardScaler(), SVC(C=20, gamma=0.1)),\n",
        "]:\n",
        "\n",
        "    print(\n",
        "        \"{:.3f}\".format(\n",
        "            cross_val_score(p, features, labels, scoring=\"accuracy\", cv=5).mean()\n",
        "        ),\n",
        "        end=\" \",\n",
        "    )\n",
        "    print([pi[1] for pi in p.steps])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Optional exercises\n",
        "\n",
        "1. Build a classification pipeline to classifiy the 2D xor- and circle data sets with linear classifiers. Also assess their performance.\n",
        "\n",
        "2. Build a classification pipeline for the digits data set. This data set was described in the first script where we've shown how to flatten images to feature vectors. Experiment with `PCA` preprocessing, `ScandardScaler` and `SVC`. Show a few of the images with correct and incorrect labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "def check_pipelines(data):\n",
        "    features = data.iloc[:, :-1]\n",
        "    labels = data.iloc[:, -1]\n",
        "\n",
        "    for p in [\n",
        "        make_pipeline(StandardScaler(), LogisticRegression()),\n",
        "        make_pipeline(StandardScaler(), LinearSVC()),\n",
        "        make_pipeline(StandardScaler(), PolynomialFeatures(2), LogisticRegression()),\n",
        "        make_pipeline(StandardScaler(), PolynomialFeatures(2), LinearSVC()),\n",
        "    ]:\n",
        "\n",
        "        print(\n",
        "            \"{:.3f}\".format(\n",
        "                cross_val_score(p, features, labels, scoring=\"accuracy\", cv=5).mean()\n",
        "            ),\n",
        "            end=\" \",\n",
        "        )\n",
        "        print([pi[1] for pi in p.steps])\n",
        "\n",
        "\n",
        "print(\"#### XOR\")\n",
        "xor_data = pd.read_csv(\"data/xor.csv\")\n",
        "check_pipelines(xor_data)\n",
        "print()\n",
        "print(\"#### Circle\")\n",
        "circle_data = pd.read_csv(\"data/circle.csv\")\n",
        "check_pipelines(circle_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# classifying digits\n",
        "\n",
        "data_set = load_digits()\n",
        "features = data_set.data\n",
        "labels = data_set.target\n",
        "\n",
        "for p in [\n",
        "    make_pipeline(StandardScaler(), SVC()),\n",
        "    make_pipeline(PCA(10), SVC()),\n",
        "    make_pipeline(StandardScaler(), PCA(10), SVC()),  # no? pixels are already scaled\n",
        "    make_pipeline(\n",
        "        PCA(10), StandardScaler(), SVC()\n",
        "    ),  # no? already scaled, even after PCA\n",
        "    make_pipeline(PCA(15), SVC()),\n",
        "    make_pipeline(PCA(17), SVC()),\n",
        "    make_pipeline(PCA(20), SVC()),\n",
        "    make_pipeline(PCA(17), SVC(C=0.5)),\n",
        "    make_pipeline(PCA(17), SVC(C=2)),\n",
        "    make_pipeline(PCA(17), SVC(C=4)),\n",
        "    make_pipeline(PCA(17), SVC(C=5)),\n",
        "    make_pipeline(PCA(17), StandardScaler(), SVC(C=4)),  # no? no, just a sanity check\n",
        "]:\n",
        "\n",
        "    print(\n",
        "        \"{:.3f}\".format(\n",
        "            cross_val_score(p, features, labels, scoring=\"accuracy\", cv=5).mean()\n",
        "        ),\n",
        "        end=\" \",\n",
        "    )\n",
        "    print([pi[1] for pi in p.steps])\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split 80:20 with fixed randomization:\n",
        "(features_train, features_test, labels_train, labels_test) = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "p.fit(features_train, labels_train)\n",
        "predicted = p.predict(features_test)\n",
        "incorrect = np.where(predicted != labels_test)[0]\n",
        "\n",
        "print(\"#### Incorrect\")\n",
        "print(incorrect)\n",
        "\n",
        "\n",
        "def show(indices):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        plt.subplot(1, len(indices), i + 1)\n",
        "        img = features_test[idx].reshape(8, 8)\n",
        "        plt.imshow(\n",
        "            img,\n",
        "            cmap=\"gray\",\n",
        "        )\n",
        "        plt.title(\"is {}, predicted as {}\".format(labels_test[idx], predicted[idx]))\n",
        "\n",
        "\n",
        "show(incorrect)\n",
        "\n",
        "# there are many correct ones, so let's pick a sample of size of the numer of incorrect ones\n",
        "import random\n",
        "\n",
        "correct = np.where(predicted == labels_test)[0]\n",
        "correct = random.sample(\n",
        "    list(correct), len(incorrect)\n",
        ")  # from np array to list, this is required!\n",
        "show(correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"><p>\n",
        "<i class=\"fa fa-info-circle\"></i>&nbsp;\n",
        "Up to now we've applied preprocessing only to the full features table. <strong>To preprocess single columns or a subset of them, e.g. to apply function transformers, or to input missing values, or to encode categorical columns use a <code>ColumnTransformer</code> utility</strong>. A good overview is given in <a href=\"https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\">a tutorial on applying <code>ColumnTransformer</code>s to mixed-type columns</a>.\n",
        "</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reusing pipelines: persist on a disk\n",
        "\n",
        "Learning and finding a good performing pipeline can be time intensive. It makes sense to **store and reuse pipeline later for predictions**.\n",
        "\n",
        "To that end, **use a standard library Python module `pickle` to serialize and deserialize a pipeline object** (or any other \"serializable\" Python object, including a classifier itself, or hyper-parameters search result object):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import tempfile\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "beer_data = pd.read_csv(\"data/beers.csv\")\n",
        "\n",
        "features = beer_data.iloc[:, :-1]\n",
        "labels = beer_data.iloc[:, -1]\n",
        "\n",
        "\n",
        "p = make_pipeline(StandardScaler(), PCA(3), SVC())\n",
        "p.fit(features, labels)\n",
        "print(\"SVC learnt intercept:\\n\", p.named_steps[\"svc\"].intercept_)\n",
        "\n",
        "\n",
        "# \".pkl\" is the standard file extension for \"pickled\" objects\n",
        "p_path = os.path.join(tempfile.mkdtemp(), \"pipeline.pkl\")\n",
        "\n",
        "\n",
        "# open file to _w_rite _b_inary data\n",
        "with open(p_path, \"wb\") as p_file:\n",
        "    pickle.dump(p, p_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# open file to _r_ead _b_inary data\n",
        "with open(p_path, \"rb\") as p_file:\n",
        "    p_loaded = pickle.load(p_file)\n",
        "print(\"SVC learnt intercept (loaded):\\n\", p.named_steps[\"svc\"].intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Caching pipeline data transformers\n",
        "\n",
        "Another common case is **reusing preprocessing steps that are already fitted to data - the data transformers**. This is useful when trying out pipeline composition variations, or when optimizing hyper-parameters of the classifier itself.\n",
        "\n",
        "To reuse fitted data transformers (preprocessors, decomposition etc), you can **cache pipeline on a disk using `memory=...` argument of the pipeline constructor**.\n",
        "\n",
        "Whenever you call `.fit(...)` with different inputs the pipeline transformers will be cached:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import tempfile\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# Utility function to recursively get file names in a folder.\n",
        "def get_fns_rec(path):\n",
        "    for f in glob.glob(f\"{path}/**/*.*\", recursive=True):\n",
        "        yield os.path.relpath(f, path)\n",
        "\n",
        "\n",
        "tempdir = tempfile.mkdtemp()\n",
        "print(\"Caching to directory:\", tempdir)\n",
        "\n",
        "# memory argument takes a path to a directory\n",
        "# (or a `joblib.Memory` object thar represent a directory)\n",
        "p = make_pipeline(StandardScaler(), PCA(3), SVC(), memory=tempdir)\n",
        "\n",
        "print(\"Files before fit:\")\n",
        "pprint(list(get_fns_rec(tempdir)))\n",
        "print()\n",
        "\n",
        "p.fit(features, labels)\n",
        "print(\"Files after fit 1:\")\n",
        "pprint(list(get_fns_rec(tempdir)))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each data transformer in our pipeline has a corresponding:\n",
        "* a \"pickle\" (binary `.pkl` file) containing serialized transformer object, and\n",
        "* a metadata file (human-readable `.json` file) containing 1) description of the transformer constructor and 2) input data used for training - both of these identify a cached transformer.\n",
        "\n",
        "Cache is extended on fit with different input data and re-used on fit with the same input data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Files after fit 2:\")\n",
        "# alter input data by dropping first 10 samples\n",
        "p.fit(features.iloc[:-10, :], labels.iloc[:-10])\n",
        "pprint(list(get_fns_rec(tempdir)))\n",
        "print()\n",
        "\n",
        "# if inputs are the same - cache will be used\n",
        "print(\"Files after fit 1 repeat:\")\n",
        "p.fit(features, labels)\n",
        "pprint(list(get_fns_rec(tempdir)))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"><p>\n",
        "    <i class=\"fa fa-info-circle\"></i>&nbsp;<strong>Beware</strong> that creating a pipeline with <code>memory=...</code> argument clones the transformer objects passed in a constructor. In turn, to inspect fitted transformers you must use <code>steps</code> or <code>named_steps</code> attribute of the pipeline object and not a previous explicit reference to a transformer used in pipeline (in particular, previous references won't be fitted after the pipeline was fitted).\n",
        "</p></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "\n",
        "## Take-home message: Pipelines\n",
        "\n",
        "- **scikit-learn API**: All classifiers implement `.fit` and `.predict` methods, all transformers implement `.fit`, `.transform` and `.fit_transform` methods &rarr; allows easy exchange of transformers and classifiers.\n",
        "- **Building blocks of pipeline**: 0 or more data transformers plus a final classifier.\n",
        "- **Why and how can pipelines be reused?**: To save time pipelines can be stored for evaluiation using the standard library `pickle` for serialization and deserialization.\n",
        "- **When is caching of data pipelines useful**: When reusing preprocessing steps already fitted to data.\n",
        "    \n",
        "<p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter optimization\n",
        "\n",
        "Classifiers and pipelines have parameters which must be adapted for improving performance (e.g. `gamma` or `C`). Finding good parameters is also called *hyperparameter optimization* to distinguish from the optimization done during learning of many classification algorithms.\n",
        "\n",
        "<br/>\n",
        "<div style=\"font-size: 120%;\">Up to now we adapted such hyperparameters manually, but there are more systematic approaches !</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grid search\n",
        "\n",
        "The simplest approach is to **specify valid values for each hyperparameter involved in training and then try out all possible combinations**. This is called *grid search*. In `scikit-learn` grid search is by default combined with cross-validation to asses classifier's performance for each hyperparameter set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "beer_data = pd.read_csv(\"data/beers.csv\")\n",
        "\n",
        "features = beer_data.iloc[:, :-1]\n",
        "labels = beer_data.iloc[:, -1]\n",
        "\n",
        "svc = SVC()\n",
        "\n",
        "# optimize hyperparameters of one single classifier\n",
        "\n",
        "# the keys in the dictionary match the argument names for SVC:\n",
        "parameters = {\"kernel\": (\"linear\", \"rbf\", \"poly\"), \"C\": [1, 5, 10, 15]}\n",
        "\n",
        "# run gridsearch, use CV to assess quality and determine best parameter\n",
        "# set based on classification accuracy score:\n",
        "\n",
        "# tries all 3 x 4 = 12 combinations of hyperparameter values:\n",
        "search = GridSearchCV(svc, parameters, cv=4, scoring=\"accuracy\")\n",
        "search.fit(features, labels)\n",
        "\n",
        "print(search.best_score_, search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Such an optimization can also be applied to a full pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "\n",
        "# using temporary transfomers caching for a bit better speed\n",
        "cache_dir = tempfile.mkdtemp()\n",
        "\n",
        "p = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    PolynomialFeatures(),\n",
        "    LogisticRegression(max_iter=10000),\n",
        "    memory=cache_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The specification of the grid id now a bit more complicated `PROCESSOR__ARGUMENT`: \n",
        "\n",
        "- first the name of the processor / classifier in lower case letters,\n",
        "- then two underscores `__`,\n",
        "- finally the name of the argument of the processor / classifier.\n",
        "\n",
        "`StandardScaler` e.g. has parameters `with_mean` and `with_std` which can be `True` or `False`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    \"standardscaler__with_mean\": [True, False],\n",
        "    \"standardscaler__with_std\": [True, False],\n",
        "    \"polynomialfeatures__degree\": [1, 2, 3, 4],\n",
        "    \"logisticregression__C\": [0.01, 0.1, 1, 10, 100],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This grid has `4 x 2 x 2 x 5` thus `80` points. So we muss run crossvalidation for 80 different classifiers.\n",
        "\n",
        "To speed this up, we can specify `n_jobs = 2` to use `2` extra processor cores to run gridsearch in parallel (you might want to use more cores depending on your computer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "search = GridSearchCV(\n",
        "    p,\n",
        "    param_grid,\n",
        "    cv=4,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=2,\n",
        ")\n",
        "search.fit(features, labels)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have more complicated pipelines, you also can assign explicitly names to the steps which can be used in the parameter grid. However, you need to use directly the `Pipeline(...)` constructor to do so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "p_names = Pipeline(\n",
        "    steps=[\n",
        "        (\"scale\", StandardScaler()),\n",
        "        (\"poly\", PolynomialFeatures()),\n",
        "        (\"clf\", LogisticRegression(max_iter=10000)),\n",
        "    ],\n",
        "    # memory=cache_dir,\n",
        ")\n",
        "\n",
        "param_grid_short_names = {\n",
        "    \"scale__with_mean\": [True, False],\n",
        "    \"scale__with_std\": [True, False],\n",
        "    \"poly__degree\": [1, 2, 3, 4],\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
        "}\n",
        "\n",
        "import time\n",
        "\n",
        "last = 1.0\n",
        "for n_jobs in (2, 3, 4, 5, 6, 7):\n",
        "\n",
        "    s = time.time()\n",
        "    for _ in range(5):\n",
        "        search = GridSearchCV(\n",
        "            p_names,\n",
        "            param_grid_short_names,\n",
        "            cv=4,\n",
        "            scoring=\"accuracy\",\n",
        "            n_jobs=n_jobs,\n",
        "        )\n",
        "\n",
        "        search.fit(features, labels)\n",
        "    needed = time.time() - s\n",
        "\n",
        "    print(f\"{n_jobs} {needed:7.1f} s   {needed/last:5.2f}\")\n",
        "    last = needed\n",
        "\n",
        "# using a built-in notebook line magic to measure the fit time\n",
        "%time search.fit(features, labels)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random search\n",
        "\n",
        "These grid searches took some time. A less systematic but a much more efficient approach for a rather well performing classifier is *random search*.\n",
        "\n",
        "In this case, instead of a whole grid, we **specify probability distributions for the hyperparameters to draw random samples from**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import loguniform, randint\n",
        "\n",
        "param_dist_short_names = {\n",
        "    \"scale__with_mean\": [True, False],  # random value from explicit set of values\n",
        "    \"scale__with_std\": [True, False],\n",
        "    \"poly__degree\": randint(1, 4),  # random integer from 1 to 4\n",
        "    \"clf__C\": loguniform(0.01, 100),  # log random number from .01 to 100\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run now 30 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    p_names,\n",
        "    param_dist_short_names,\n",
        "    cv=4,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=2,\n",
        "    n_iter=30,\n",
        "    random_state=21,  # fix randomization for reproduciblity\n",
        ")\n",
        "%time search.fit(features, labels)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<p><i class=\"fa fa-info-circle\"></i>\n",
        "Hyperparameter search methods also \"behave\" like a single classifier - they implement <code>.fit()</code> and <code>.predict()</code> methods (*).</p>\n",
        "</div>\n",
        "\n",
        "<div>\n",
        "<p>(*) Prediction is done with the best parameters found. The underlying model or pipeline with the best parameters is available via <code>.best_estimator_</code> property. Importantly, the <strong>refit with the best parameters is done at the end</strong> of the CV-based search, <strong>using a whole training data set</strong>.</p>\n",
        "    \n",
        "<p style=\"font-size: 80%;\">The automatic refitting can be disabled by passing <code>refit=False</code> argument when specifying the search method. Then neither <code>.predict()</code>, nor <code>.best_estimator_</code> won't be available.</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Best estimator:\")\n",
        "print(search.best_estimator_)\n",
        "print()\n",
        "print()\n",
        "print(\"Training set accuracy:\", sum(search.predict(features) == labels) / len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Halving searches\n",
        "\n",
        "The `scikit-learn` includes also a drop-in replacements for both cross-validation searches that **start search at each hyperparameters set with only small random subset of samples and iteratively selects best results while increasing the number of samples**: `HalvingGridSearchCV` and `HalvingRandomSearchCV`. This is called a *halving search* technique.\n",
        "\n",
        "<table>\n",
        "    <tr><td><img src=\"images/halving_search.png\" width=\"70%\"/></td></tr>\n",
        "    <tr><td><center><sub>Source: <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_successive_halving_iterations.html#sphx-glr-auto-examples-model-selection-plot-successive-halving-iterations-py\">https://scikit-learn.org/stable/auto_examples/model_selection/plot_successive_halving_iterations.html#sphx-glr-auto-examples-model-selection-plot-successive-halving-iterations-py</a></sub></center></td></tr>\n",
        "</table>\n",
        "\n",
        "The number of samples is just a default *resource* to increase in iterations. Resource can specified alternatively, e.g., as a number of iterations that the final classifier runs while learning.\n",
        "\n",
        "For large hyperparameter space/grid the results usually are as accurate as for standard searches, but found in much less time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# halving searches are still an experimental feature (scikit-learn 1.1.2)\n",
        "# => needs to be explicitly enabled via following import:\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "\n",
        "search = HalvingGridSearchCV(\n",
        "    p_names,\n",
        "    param_grid_short_names,\n",
        "    cv=4,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=2,\n",
        "    factor=2,  # default is to actually third, not halve\n",
        "    random_state=21,  # fix randomization for reproduciblity\n",
        ")\n",
        "%time search.fit(features, labels)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "\n",
        "## Take-home message: Hyperparameter optimization\n",
        "\n",
        "- **Grid search**: Test all combinations of specified values of hyperparameters.\n",
        "- **Random search**: Specify probability distributions per hyperparameter and draw random samples.\n",
        "- **Halving searches**: Start with small number of samples and all candidates and over iterations reduce number of candidates (always choose best ones) and increase number of samples until only one candidate is left.\n",
        "\n",
        "<p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise session\n",
        "\n",
        "1. Try to find good parameters for the following two pipelines applied to the beer data set. Use grid search for the first one and randomized search for the second one. Cache the data transformers while doing the grid search. Does it pay-off to cache the data transformers during randomized search? Save the search results to the disk.\n",
        "\n",
        "    `make_pipeline(StandardScaler(), SVC(gamma=..., C=...), memory=...)`\n",
        "    \n",
        "    `make_pipeline(StandardScaler(), PolynomialFeatures(degree=...), PCA(n_components=...), LogisticRegression(C=...))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import tempfile\n",
        "\n",
        "cache_dir = tempfile.mkdtemp()\n",
        "\n",
        "\n",
        "beer_data = pd.read_csv(\"data/beers.csv\")\n",
        "\n",
        "features = beer_data.iloc[:, :-1]\n",
        "labels = beer_data.iloc[:, -1]\n",
        "\n",
        "\n",
        "p = make_pipeline(StandardScaler(), SVC(), memory=cache_dir)\n",
        "\n",
        "param_grid = {\n",
        "    \"standardscaler__with_mean\": [True, False],\n",
        "    \"standardscaler__with_std\": [True, False],\n",
        "    \"svc__C\": [1, 10, 15, 20, 25],\n",
        "    \"svc__gamma\": [0.01, 0.05, 0.1, 0.5, 1],\n",
        "}\n",
        "\n",
        "search = GridSearchCV(p, param_grid, cv=5, scoring=\"accuracy\", n_jobs=5)\n",
        "search.fit(features, labels)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)\n",
        "\n",
        "search_path = os.path.join(cache_dir, \"search_1.pkl\")\n",
        "with open(search_path, \"wb\") as search_file:\n",
        "    pickle.dump(search, search_file)\n",
        "\n",
        "\n",
        "from scipy.stats import randint, uniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "p = make_pipeline(\n",
        "    StandardScaler(), PolynomialFeatures(), PCA(), LogisticRegression(max_iter=100_000)\n",
        ")\n",
        "param_grid = {\n",
        "    \"polynomialfeatures__degree\": randint(2, 4),\n",
        "    \"pca__n_components\": randint(2, 16),\n",
        "    \"logisticregression__C\": loguniform(0.1, 100),\n",
        "}\n",
        "# Note: not using cache w/ randomized search for this pipeline.\n",
        "#       Only the fast StandardScaler would benefit from caching;\n",
        "#       overhead for caching all transformers w/ random values is bigger.\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    p,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=5,\n",
        "    random_state=42,  # fix randomization for reproduciblity\n",
        ")\n",
        "search.fit(features, labels)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)\n",
        "\n",
        "search_path = os.path.join(cache_dir, \"search_2.pkl\")\n",
        "with open(search_path, \"wb\") as search_file:\n",
        "    pickle.dump(search, search_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional Exercises\n",
        "\n",
        "Using random search optimize parameters for pipelines for:\n",
        "\n",
        "- the spiral data set (`SVC`)\n",
        "- the digits data set (`PCA` + `SVC`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.stats import loguniform\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "data = pd.read_csv(\"data/spiral.csv\")\n",
        "\n",
        "features = data.iloc[:, :-1]\n",
        "labels = data.iloc[:, -1]\n",
        "\n",
        "# note: data is scaled already\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(features.iloc[:, 0], features.iloc[:, 1], color=[\"rb\"[l] for l in labels])\n",
        "labels = data.iloc[:, -1]\n",
        "\n",
        "clf = SVC()\n",
        "\n",
        "param_grid = {\n",
        "    \"C\": loguniform(0.01, 100),\n",
        "    \"gamma\": loguniform(0.01, 100),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=5,\n",
        "    random_state=21,  # fix randomization for reproduciblity\n",
        ")\n",
        "search.fit(features, labels)\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "\n",
        "import random\n",
        "\n",
        "from scipy.stats import loguniform, randint, uniform\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# classifying digits\n",
        "\n",
        "data_set = load_digits()\n",
        "features = data_set.data\n",
        "labels = data_set.target\n",
        "\n",
        "\n",
        "param_dist = {\n",
        "    \"pca__n_components\": randint(15, 20),\n",
        "    \"svc__C\": uniform(4, 6),\n",
        "    \"svc__gamma\": loguniform(1e-5, 1e-3),\n",
        "}\n",
        "\n",
        "p = make_pipeline(PCA(), SVC())\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    p,\n",
        "    param_dist,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=4,\n",
        "    n_iter=40,\n",
        "    random_state=21,  # fix randomization for reproduciblity\n",
        ")\n",
        "search.fit(features, labels)\n",
        "\n",
        "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
        "print(search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (C) 2019-2023 ETH Zurich, SIS ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}