{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"X does not have valid feature names, but [a-zA-Z]+ was fitted with feature names\",\n",
        "    category=UserWarning,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings = lambda *a, **kw: None\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "HTML(open(\"custom.html\", \"r\").read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: Metrics for evaluating the performance of a classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Up to now we used *accuracy*, the percentage of correct classifcations, to evaluate the quality of a classifier.\n",
        "\n",
        "Regrettably _accuracy_ can produce very misleading results. \n",
        "\n",
        "This chapter will discuss other metrics used to asses the quality of a classifier, including the possible pitfalls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  The confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we define the **confusion matrix** we must introduce some additional terms. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After applying a classifier to a data set with known labels `0` and `1`:\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</div>\n",
        "<ul>\n",
        "\n",
        "<li><strong>TP (true positives)</strong>: labels which were predicted as <code>1</code> and actually are <code>1</code>. <br/><br/>\n",
        "\n",
        "\n",
        "<li><strong>TN (true negatives)</strong>: labels which were predicted as <code>0</code> and actually are <code>0</code>.<br/><br/>\n",
        "\n",
        "\n",
        "<li><strong>FP (false positives)</strong>: labels which were predicted as <code>1</code> and actually are <code>0</code>.<br/><br/>\n",
        "\n",
        "\n",
        "<li><strong>FN (false negatives)</strong>: labels which were predicted as <code>0</code> and actually are <code>1</code>.<br/><br/>\n",
        "\n",
        "</ul>\n",
        "\n",
        "To memorize this: \n",
        "\n",
        "<ul>\n",
        "\n",
        "<li>The second word \"positives\"/\"negatives\" refers to the prediction computed by the classifier.\n",
        "<li>The first word \"true\"/\"false\" expresses if the classification was correct or not.\n",
        "\n",
        "</ul>\n",
        "\n",
        "This is the so called <strong>Confusion Matrix</strong>:\n",
        "\n",
        "<table style=\"border: 1px; font-family: 'Source Code Pro', monocco, Consolas, monocco, monospace;\n",
        "              font-size:110%;\">\n",
        "    <tbody >\n",
        "        <tr>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\"> </td>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual P</td>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual N</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted P</td>\n",
        "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TP         </td>\n",
        "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FP         </td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted N</td>\n",
        "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FN         </td>\n",
        "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TN         </td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./images/305c8j.jpg\" title=\"made at imgflip.com\" width=40%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "- So the total number of predictions can be expressed as `TP` + `FP` + `FN` + `TN`.\n",
        "\n",
        "\n",
        "- The number of correct predictions is `TP` + `TN`.\n",
        "\n",
        "\n",
        "- `TP` + `FN` is the number of positive examples in our data set, \n",
        "\n",
        "\n",
        "- `FP` + `TN` is the number of negative examples.\n",
        "\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</div>\n",
        "\n",
        "This allows us to define <strong>accuracy</strong> as (<code>TP</code> + <code>TN</code>) / (<code>TP</code> + <code>FP</code> + <code>FN</code> + <code>TN</code>).\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pitfalls\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<i class=\"fa fa-info-circle\"></i>&nbsp; Accuracy can be very misleading if class sizes are imbalanced\n",
        "</div>\n",
        "\n",
        "\n",
        "Let us demonstrate this with an extreme example:\n",
        "\n",
        "- On average 10 out of 10000 people are infected with a disease `X`. \n",
        "- A medical test `Z` diagnoses 50 % of infected people as `not infected` ?\n",
        "- The test is correct on all  not-infected people.\n",
        "\n",
        "\n",
        "Among $10000$ people \n",
        "\n",
        "- $10$ will be infected, $5$ gets a correct result.\n",
        "- $9990$ will be not infected with a correct test result.\n",
        "\n",
        "Thus accuracy is $\\frac{9995}{10000} = 99.95 \\% $\n",
        "\n",
        "\n",
        "This is also called the **accuracy paradox** (<a href=\"https://en.wikipedia.org/wiki/Accuracy_paradox\">see also here</a>)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./images/303wyp.jpg\" title=\"made at imgflip.com\" width=50%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate this test on such an unbalanced dataset we need different numbers: \n",
        "\n",
        "1. Does our test miss infected people: How many infected people are actually discovered to be infected ?\n",
        "\n",
        "2. Does our test predict people as infected which are actually not: How many positive diagnoses are correct ?\n",
        "\n",
        "We come back to this example later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise block 1\n",
        "\n",
        "1.  A classifier predicts labels `[0, 1, 0, 1, 1, 0, 1, 0]` whereas true labels are `[0, 0, 1, 1, 1, 0, 1, 1]`. First write these values as a two columned table using pen & paper and assign `FP`, `TP`, ... to each row. Now create the confusion matrix and compute accuracy.\n",
        "\n",
        "2. A random classfier just assign a randomly chosen label `0` or `1` to a given sample. What is the average accuracy of such a classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "source": [
        "SOLUTION 1.1 \n",
        "<pre>\n",
        "TRUE   PREDICTED   THIS IS\n",
        "0      0           TN\n",
        "0      1           FP \n",
        "1      0           FN\n",
        "1      1           TP\n",
        "1      1           TP\n",
        "0      0           TN\n",
        "1      1           TP\n",
        "1      0           FN\n",
        "\n",
        "TP = 3    FP = 1\n",
        "FN = 2    TN = 2\n",
        "\n",
        "accuracy = 5 / 8 = 62.5 %\n",
        "</pre>\n",
        "\n",
        "SOLUTION 1.2 \n",
        "\n",
        "On average all fields of the confusion matrix should contain same values, thus the accuracy would be 50 %."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional exercise\n",
        "\n",
        "Assume the previously described test also produces wrong results on not-infected people, such that 5% will be diagnosed as infected. Compute the confusion matrix and the accuracy of this test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "source": [
        "SOLUTION Optional exercise\n",
        "\n",
        "This is the new situation:\n",
        "- On average 10 out of 10000 people are infected with a disease `X`. \n",
        "- A medical test `Z` diagnoses 50 % of infected people as `not infected` ?\n",
        "- The test is correct on 95% of all people.\n",
        "\n",
        "<pre>\n",
        "Infected people     = 10,      diagnosed as X: 5 (TP)\n",
        "Not infected people = 9990,    diagnosed as X: 0.05 * 9990 = 499.5 (FP)\n",
        "\n",
        "TP = 5   FP = 499.5\n",
        "FN = 5   TN = 9490.5\n",
        "\n",
        "accuracy = 9495.5 / 10000 = 94.96 %\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Precision and Recall\n",
        "\n",
        "\n",
        "A few days before thanksgiving you open an online recipe website and enter \"turkey thanksgiving\". You see some suitable recommendations but also unusable results related to Turkish recipes.\n",
        "\n",
        "Such a search engine works like a filter applied on a collection of documents.\n",
        "\n",
        "As a scientist you want to assess the reliablity of this service:\n",
        "\n",
        "1. What fraction of relevant recipes stored in the underlying database do I see?\n",
        "\n",
        "2. How many of the shown results are relevant recipes and not the recipes from Turkey?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./images/precision-recall-1.png\" width=90% />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to compute precision and recall for a classifier\n",
        "\n",
        "To transfer this concept to classification, we can interpret a classifier as a filter. The classifier classifies every  document in a collection as relevant or not relevant.\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</div>\n",
        "\n",
        "To remember:\n",
        "\n",
        "<table style=\"border: 1px; font-family: 'Source Code Pro', monocco, Consolas, monocco, monospace;\n",
        "              font-size:110%;\">\n",
        "    <tbody >\n",
        "        <tr>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\"> </td>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual P</td>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual N</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted P</td>\n",
        "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TP         </td>\n",
        "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FP         </td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted N</td>\n",
        "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FN         </td>\n",
        "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TN         </td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</div>\n",
        "\n",
        "<img src=\"./images/precision-recall-2.png\" width=90% />\n",
        "\n",
        "<div style=\"font-size: 130%; font-weight: bold;\">Example</div>\n",
        "\n",
        "The confusion matrix for the medical test `Z` is then:\n",
        "\n",
        "\n",
        "<table style=\"border: 1px solid black\">\n",
        "    <tr style=\"border: 1px black\">\n",
        "        <td style=\"border: 1px  solid black; background: white; padding: 1em\">TP = 5</td>\n",
        "        <td style=\"border: 1px  solid black; background: white; \">FP = 0</td>\n",
        "    </tr>\n",
        "    <tr style=\"border: 1px black\">\n",
        "        <td style=\"border: 1px solid black; background: white; padding: 1em \">FN = 5</td>\n",
        "        <td style=\"border: 1px solid black; background: white; \">TN = 9990</td>\n",
        "    </tr>\n",
        "        \n",
        "</table>\n",
        "\n",
        "Here precision is `1.0` and recall is `0.5`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trade-off between precision and recall.\n",
        "\n",
        "We can **increase recall** by \"opening\" the classifier to show more results. In case of a linear classifier this could be done by decreasing the threshold.\n",
        "\n",
        "The downside is that this would also increase the number of irrelevant results, thus **reduce precision**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F1-score\n",
        "\n",
        "Sometimes we want a single number instead of two numbers to rank the performaces of multiple classifiers.\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</div>\n",
        "    \n",
        "The **F1 score** is computed as\n",
        "<code>F1 = 2 * (precision * recall) / (precision + recall)</code>.\n",
        "\n",
        "This is the *harmonic mean* of precision and recall.\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "For the medical test `Z` the `F1` score is `1 / 1.5 = 0.6666..`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise block 2\n",
        "\n",
        "Use your results from Exercise block 1.1 to compute precision, recall and F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "source": [
        "SOLUTION 2\n",
        "<pre>\n",
        "TP = 3    FP = 1\n",
        "FN = 2    TN = 2\n",
        "\n",
        "precision = 3 / (3 + 1) = 75 %\n",
        "recall    = 3 / (3 + 2) = 60 %\n",
        "F1        = 2 * (0.6 * 0.75) / (0.6 + 0.75) = 66.66%\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional exercise:\n",
        "\n",
        "Compute precision, recall and F1-score for the test described in Exercise block 1 Optional exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "source": [
        "SOLUTION 2 Optional exercise\n",
        "\n",
        "<pre>\n",
        "TP = 5   FP = 499.5\n",
        "FN = 5   TN = 9490.5\n",
        "\n",
        "precision = 5 / (5 + 499.5) = 0.0099\n",
        "recall    = 5 / (5 + 5) = 0.5\n",
        "F1        = 2 * (0.0099 * 0.5) / (0.0099 + 0.5) = 0.0194\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other metrics\n",
        "\n",
        "The discussion above was just a quick introduction to measuring the accuracy of a classifier. We skipped other metrics such as `ROC` and `AUC` amongst others.\n",
        "\n",
        "A good introduction to `ROC` <a href=\"https://classeval.wordpress.com/introduction/introduction-to-the-roc-receiver-operating-characteristics-plot/\">can be found here.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics in scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`sklearn.metrics` contains all introduced above metrics, as well as the previously-used classification accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# these numbers are from exercise 1.1:\n",
        "predicted = [0, 1, 0, 1, 1, 0, 1, 0]\n",
        "labels = [0, 0, 1, 1, 1, 0, 1, 1]\n",
        "\n",
        "\n",
        "# The first argument of the metrics functions is the exact labels,\n",
        "# the second argument is the predictions:\n",
        "#\n",
        "\n",
        "print(\"{:20s} {:.3f}\".format(\"precision\", precision_score(labels, predicted)))\n",
        "print(\"{:20s} {:.3f}\".format(\"recall\", recall_score(labels, predicted)))\n",
        "print(\"{:20s} {:.3f}\".format(\"f1\", f1_score(labels, predicted)))\n",
        "print(\"{:20s} {:.3f}\".format(\"accuracy\", accuracy_score(labels, predicted)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification report\n",
        "\n",
        "`scikit-learn` also offers a function to print a classification report, which is an overview table of precision, recall and F1 metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\n",
        "    classification_report(\n",
        "        labels,\n",
        "        predicted,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. The `support` column lists the number of samples in each class and in total.\n",
        "2. The `macro average` row lists unweighted mean of a metric for each label. This does NOT take classes imbalance into account.\n",
        "3. The `weighted average` row lists weighted by support mean of a metric for each label. This does take classes imbalance into account.\n",
        "\n",
        "Note: normally the precision, recall and F1 metrics are only the \"Positive\" (`1`) class metrics (cf. results above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `sklearn.metrics` module contains also `confusion_matrix` utility which returns the confusion matrix.\n",
        "\n",
        "Beware: the matrix is transposed with respect to the conventional notation; actual (true) classes are given in rows, whereas predicted in columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# print confusion matrix nicely using pandas DataFrame\n",
        "print(\n",
        "    pd.DataFrame(\n",
        "        # pass classes, here called labels, otherwise sort order (0, 1) will be used\n",
        "        confusion_matrix(labels, predicted, labels=[1, 0]),\n",
        "        # actual (true) classes are given in rows, whereas predicted in columns\n",
        "        index=[\"Actual P\", \"Actual N\"],\n",
        "        columns=[\"Predicted P\", \"Predicted N\"],\n",
        "    )\n",
        ")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having a classifier object, the confusion matrix can also be visualized using a `ConfusionMatrixDisplay` utility class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "beer_data = pd.read_csv(\"data/beers.csv\")\n",
        "\n",
        "features = beer_data.iloc[:, :-1]\n",
        "labels = beer_data.iloc[:, -1]\n",
        "\n",
        "(\n",
        "    features_train,\n",
        "    features_test,\n",
        "    labels_train,\n",
        "    labels_test,\n",
        ") = train_test_split(features, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "\n",
        "classifier = LogisticRegression(C=1)\n",
        "classifier.fit(features_train, labels_train)\n",
        "\n",
        "cm_disp = ConfusionMatrixDisplay.from_estimator(\n",
        "    estimator=classifier,\n",
        "    X=features_test,\n",
        "    y=labels_test,\n",
        "    display_labels=classifier.classes_,\n",
        "    cmap=plt.cm.Blues,\n",
        ")\n",
        "\n",
        "cm_disp.ax_.set_title('Confusion matrix: \"beer\" dataset + LR classfier')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<p>\n",
        "<i class=\"fa fa-info-circle\"></i>\n",
        "The confusion matrix generalizes directly to multiple classes (i.e. more than binary \"Positive\" and \"Negative\" class). For instance, using <code>sklearn.datasets.load_iris</code> with vanilla <code>SVC</code> one gets the following confusion matrix plot:\n",
        "</p>\n",
        "    \n",
        "<img src=\"./images/confusion_matrix-iris_svc.png\" width=\"50%\" />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Case-study: an imbalanced dataset\n",
        "\n",
        "The function `cross_val_score` (introduced in the last script) allows to use other metrics than `accuracy`.\n",
        "\n",
        "We demonstrate usage of different metrics on two data sets:\n",
        "\n",
        "- the beer data samples in which labels distribution is almost 50:50, and\n",
        "- an unbalanced subset of the beer data samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "beer_data = pd.read_csv(\"data/beers.csv\")\n",
        "print(\"balanced data\")\n",
        "print(beer_data.shape)\n",
        "print(\"#class 1:\", sum(beer_data.iloc[:, -1] == 1))\n",
        "print()\n",
        "\n",
        "# we sort by label, then removing samples of one class is easy:\n",
        "beer_data = beer_data.sort_values(by=\"is_yummy\")\n",
        "\n",
        "beer_data_unbalanced = beer_data.iloc[:-80, :]\n",
        "\n",
        "print(\"unbalanced data\")\n",
        "print(beer_data_unbalanced.shape)\n",
        "print(\"#class 1:\", sum(beer_data_unbalanced.iloc[:, -1] == 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def assess(classifier, beer_data):\n",
        "    features = beer_data.iloc[:, :-1]\n",
        "    labels = beer_data.iloc[:, -1]\n",
        "    n = len(labels)\n",
        "    print(\"{:.1f} % of the beers are yummy\".format(100 * sum(labels == 1) / n))\n",
        "    print()\n",
        "\n",
        "    # NOTE: metrics given in `cross_val_score` as strings (names).\n",
        "    #       (In order to use metric functions, as these imported from `sklearn.metrics`,\n",
        "    #       you need to transform them first into estimator scorer function using\n",
        "    #       `sklearn.metrics.make_scorer()` function, e.g. `make_scorer(f1_score)`.)\n",
        "    for metric in [\"accuracy\", \"f1\", \"precision\", \"recall\"]:\n",
        "        scores = cross_val_score(classifier, features, labels, scoring=metric, cv=5)\n",
        "        print(\"   {:12s}: mean value: {:.2f}\".format(metric, scores.mean()))\n",
        "\n",
        "    print()\n",
        "\n",
        "\n",
        "classifier = LogisticRegression(C=1)\n",
        "\n",
        "print(\"balanced data\")\n",
        "assess(classifier, beer_data)\n",
        "\n",
        "print(\"unbalanced data\")\n",
        "assess(classifier, beer_data_unbalanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that for the balanced data set the values for `f1` and for `accuracy` are almost equal, but differ significantly for the unbalanced data set. The `f1` metric captures the `precision` and `recall` trade off which is visible for imbalanced datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise block 3\n",
        "\n",
        "Play with the previous examples; for beer data try `SVC` classifier with different `C` and `gamma` settings.\n",
        "\n",
        "### Optional exercise\n",
        "\n",
        "Modify the code from section *Training the final classifier* from the previous script to find best hyperparameters for the beers data set, but this time use a `f1` metric. How did the previously best hyperparameters do now?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "solution"
        ]
      },
      "outputs": [],
      "source": [
        "# SOLUTION 3 Optional exercise\n",
        "\n",
        "beer_data = pd.read_csv(\"data/beers.csv\")\n",
        "\n",
        "# Note 1: `shuffle=True` is default, hence, unnecessary to specify\n",
        "# Note 2: using `stratify=labels` to perserve classes proportion after split same as in the original dataset\n",
        "(\n",
        "    features_crosseval,\n",
        "    features_validation,\n",
        "    labels_crosseval,\n",
        "    labels_validation,\n",
        ") = train_test_split(features, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# OPT, cf. from sklearn.metrics import classification_report\n",
        "\n",
        "# FIND A \"BEST\" CLASSIFIER\n",
        "# with fixed randomization\n",
        "\n",
        "# By default `cross_val_score(.., cv=n)` call implicitly uses\n",
        "# `KFold(n_splits=n, shuffle=False)` cross-validator\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "cross_validator = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"OPTIMIZE HYPERPARAMETERS\")\n",
        "# selected classifier hyperparameters to optimize\n",
        "SVC_C_values = (1e-1, 1, 10)\n",
        "SVC_gamma_values = (0.0001, 0.001, 0.01, 0.1)\n",
        "\n",
        "for C in SVC_C_values:\n",
        "    for gamma in SVC_gamma_values:\n",
        "        classifier = SVC(C=C, gamma=gamma)\n",
        "        test_scores = cross_val_score(\n",
        "            classifier,\n",
        "            features_crosseval,\n",
        "            labels_crosseval,\n",
        "            scoring=\"f1\",  # CHANGE\n",
        "            cv=cross_validator,\n",
        "        )  # cv arg is now different\n",
        "        print(\n",
        "            \"score = {:.3f} +/- {:.3f}, C = {:.1e},  gamma = {:.1e}\".format(\n",
        "                test_scores.mean(), test_scores.std(), C, gamma\n",
        "            )\n",
        "        )\n",
        "        results.append((test_scores.mean(), test_scores.std(), C, gamma))\n",
        "\n",
        "# max of list of tuples considers value of first entry\n",
        "# to compare tuples. This we look for test_scores.mean() value:\n",
        "\n",
        "# max of list of tuples considers value of first entry\n",
        "# to compare tuples. This we look for test_scores.mean() value:\n",
        "\n",
        "best_result = max(results)\n",
        "best_score_mean, best_score_std, best_C, best_gamma = best_result\n",
        "\n",
        "print()\n",
        "print(\"BEST RESULT CROSS VALIDATION\")\n",
        "print(\n",
        "    \"score = {:.3f} +/- {:.3f}, C = {:.1e},  gamma = {:.1e}\".format(\n",
        "        best_score_mean, best_score_std, best_C, best_gamma\n",
        "    )\n",
        ")\n",
        "\n",
        "# EVALUATE CLASSIFIER ON VALIDATION DATASET\n",
        "\n",
        "classifier = SVC(C=best_C, gamma=best_gamma)\n",
        "\n",
        "classifier.fit(features_crosseval, labels_crosseval)\n",
        "predicted = classifier.predict(features_validation)\n",
        "\n",
        "final_accuracy = sum(predicted == labels_validation) / len(labels_validation)\n",
        "\n",
        "print(\"VALIDATION\")\n",
        "print(\"score = {:.3f}\".format(final_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (C) 2019-2022 ETH Zurich, SIS ID"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}